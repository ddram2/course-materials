---
created: 2026.02.18
MOC1: MOC.10.edu.교과_콘텐츠
MOC2a: MOC.50.Ag.Chem
MOC2b: ""
source:
tags:
  - 교육/강의
  - 바이오모니터링/방법론
  - 환경보건
  - 연구
  - 교육
---
# Session 4 기조발제 — 세 가지 아젠다에 대한 논거 정리

> 작성 목적: 기조발제에서 던진 "삐딱한 주장"과 "불편한 질문"에 대해, 김성균 교수의 교육 실천 경험, AI(Claude)의 분석적 관점, 해외 대학·출판기관의 정책을 종합하여 논거를 정리한다.
> 이 문서는 발제자 참고용이며, 토론 시 근거 자료로 활용할 수 있다.

---

## 아젠다 1. AI 산출물, 어디까지 용인할 수 있을까?

### 삐딱한 주장 요약

> "AI에게 자료를 주고 산출물을 받아 copy/paste하면 안 되나? 훑어 보니 괜찮은데. 환각도 줄었고, IDE로 미세조정하면 더 정확하다. 더 중요한 일에 시간 쓰는 게 효율적 아닌가?"

---

### 1-A. 김성균 교수의 입장 (교육 실천 경험 기반)

**핵심 논지: AI 산출물의 "정확도 향상"과 "그대로 쓸 수 있는가"는 별개의 문제다.**

**(1) "오타 없는 답변과 유려한 서술"이 학습을 보증하지 않는다**

바이오모니터링 강좌에서 중간고사 대체 term paper를 AI와 협력하여 채점한 경험에 따르면, "AI가 제시하는 내용과 방향에 의존하고 있다는 인상을 지우기 힘들었다. 무엇보다 오타 없는 답변과 유려한 서술이 그러하다. 내용도 대동소이하게 Domain Knowledge를 담고 있기는 했다"고 술회한 바 있다 [김성균, AI 활용 교육 실천 분석, 2025].

이는 산출물의 표면적 품질이 높아졌을수록 오히려 검증의 필요성이 커진다는 역설을 보여준다. 30% 수정했든 70% 수정했든, 핵심 문제는 **수정 비율이 아니라 "무엇을 수정할 수 있는 역량이 있는가"**이다.

**(2) 환각이 줄었다는 주장의 함정: "대부분의 참 + 일부의 거짓"이 더 위험하다**

"현재 최신 AI 모델의 할루시네이션이 많이 줄었다고 해도 1% 내외의 거짓이 섞인 정보를 분별하는 것은 더욱 어렵다. 대부분의 참과 일부의 거짓 또는 모호함이 섞인 것은 위험한 정보가 될 수 있다" [김성균, AI 활용 대학원 교육 단상, 2025.08.15].

환각이 10%일 때는 누구나 의심한다. 1%로 줄었을 때 오히려 경계심이 무너진다. 이것이 copy/paste의 진짜 위험이다.

**(3) 글쓰기는 "지식의 지혜화" 과정이다 — 아웃소싱하면 그 과정이 사라진다**

"글쓰기는 힘든 과정이다. 내 손으로 만질 수 있는 지식을 가공해 나의 언어로 재정리하는 것이기 때문이다. AI는 이 과정을 아웃소싱하게 한다. 간편하고 글쓴이가 검토하고 최종 컨펌한다 해도, 이를 통한 정보의 지식화, 지식의 지혜화로 가는 지적 활동은 이전과 다를 것이다" [김성균, 같은 글].

---

### 1-B. AI(Claude)의 분석적 관점

**핵심 논지: "정확도"와 "이해도"는 독립적 차원이다. AI 산출물의 정확도가 100%에 수렴하더라도, 그것을 그대로 쓰면 안 되는 이유는 여전히 존재한다.**

**(1) 검증 역량의 역설 (Verification Paradox)**

여기서 "검증 역량"이란, AI가 생성한 산출물의 정확성·논리적 타당성·맥락적 적절성을 **독립적으로 판별할 수 있는 능력**을 뜻한다. AI의 답이 맞는지, 논리적 비약이 없는지, 자신의 연구 맥락에 부합하는지를 스스로 판단할 수 있어야 한다.

이것이 "역설"인 이유는 **순환적 의존성(circular dependency)**에 있다. AI 산출물을 검증하려면 해당 주제에 대한 독립적 이해가 필요한데, AI에게 작성을 위임하면 그 독립적 이해를 형성할 기회 자체가 사라진다. 즉, **검증할 역량이 없는 상태에서 검증해야 하는 상황**에 빠진다.

Bastani et al. (2025)는 이 역설을 실증적으로 보여주었다. `OOO의 개념이 뭐야?`와 같은 방식으로 GPT를 사용한 학생들은 AI가 제시한 **산술 오류(arithmetic error)와 논리 오류(logical error)를 거의 동일한 비율로 수용**했다. 산술 오류는 학생들이 충분히 검증할 수 있는 수준임에도 논리 오류와 비슷하게 그대로 복사했다는 것은, 검증 자체를 시도하지 않았다는 의미이다. 검증 역량의 문제가 아니라, AI에 의존하는 순간 **검증 동기 자체가 소멸**한 것이다 [Bastani, H., Bastani, O., Sungu, A., Ge, H., Kabakçı, Ö., & Mariman, R. (2025). Generative AI without guardrails can harm learning. *Proceedings of the National Academy of Sciences*, 122(26)].

**(2) 같은 AI, 다른 결과 — Scaffold(발판) vs. Crutch(목발)의 실증**

Mollick & Mollick (2023)은 AI를 "답을 주는 도구"가 아니라 "증거 기반 교육 전략(evidence-based teaching strategies)을 구현하는 도구"로 설계하면, 교육자의 역량을 증폭(force multiplier)시킬 수 있다는 원칙을 제시했다 [Mollick, E. R. & Mollick, L. (2023). Using AI to Implement Effective Teaching Strategies in Classrooms: Five Strategies, Including Prompts. *The Wharton School Research Paper*. SSRN: 4391243].

Bastani et al. (2025)는 이 원칙을 실험적으로 검증했다. 동일한 GPT-4를 두 가지 방식으로 설계하여 비교한 결과:

- **GPT Base (직접 사용 — Crutch)**: 학생이 질문하면 AI가 전체 답을 제공. 연습문제 성적 48% 향상, **사후 시험(AI 없는 조건) 성적 17% 하락**. 대화의 67%가 단순히 답을 요청하거나 문제를 반복하는 superficial conversation이었다.
- **GPT Tutor (안내자로 사용 — Scaffold)**: 학생이 먼저 자기 풀이를 보여주면 AI가 단계별 힌트를 제공. 연습문제 성적 127% 향상, **사후 시험 성적은 통제군과 동일** (학습 효과 유지). 학생들은 문제당 메시지 수가 더 많고, 시간도 13% 더 투자했다.
	- AI를 tutor로 사용하는 사례: 
		- "multiple examples 제공", "학생 오개념 발견", "frequent low-stakes testing", "분산 학습(Spaced practice)" 등의 **증거 기반 교육 전략(evidence-based teaching strategies)을 구현하는 도구**로 사용
			- 분산학습은 집중학습(버력치기)에서 할 수 없는 교차연습과 망각 후 인출과정을 겪어야 한다.

즉, **동일한 AI(GPT-4)라도 설계에 따라 scaffold가 될 수도, crutch가 될 수도 있다**. 이 구분이 "환각이 줄었으니 그대로 써도 된다"는 주장에 대한 직접적 반론이다. 문제는 AI의 정확도가 아니라, **사용자가 사고 과정을 AI에게 위임했는가 여부**이다.

**(3) 인과 사슬: AI 의존 → 역량 상실의 경로**

위 두 연구를 종합하면 다음과 같은 인과 사슬이 성립한다:

1. AI를 crutch로 사용 → 독립적 이해 형성 기회 상실 (Mollick의 원칙)
2. 독립적 이해 없음 → AI 산출물 검증 불가능 (검증 역량의 역설)
3. 검증 불가능 → 오류를 포함한 답을 그대로 수용 (Bastani의 실증)
4. 학습 효과 없음 → AI 없는 조건에서 성적 하락 (Bastani의 실증)

AI가 100% 정확해지더라도, 사고 과정을 위임하면 학습자는 문제를 독립적으로 분석하는 역량을 상실하고, 맥락적 판단과 비판적 사고를 발달시킬 기회를 잃는다. 이것이 **"정확도"와 "이해도"가 독립적 차원**이라는 명제의 핵심이다.

**(4) "가공"의 기준은 비율이 아니라 "지적 주도권"이다**

서울대 가이드라인의 "가공 없이 그대로"라는 표현은 수정 비율로 해석할 것이 아니다. 핵심은 **연구자가 내용의 논리적 구조, 해석, 결론에 대해 독립적으로 설명하고 방어할 수 있는가**이다. 이것이 가능하면 AI를 활용한 것이고, 불가능하면 AI에게 위임한 것이다. AI에게 위임한 지식은 자기 지식이 아니다.

---

### 1-C. 해외 대학·출판기관의 입장

**(1) 출판기관: "쓰되, 밝히라" — 단 경계가 있다**

- **Springer Nature**: "AI-assisted copy editing"(가독성, 문체, 문법 교정)은 공시 불요. 그러나 "generative AI work"(내용 생성, 초안 작성, 요약)은 반드시 공시해야 한다. AI를 저자(author)로 등재할 수 없다 [Springer Nature Editorial Policies, 2024. https://www.springernature.com/gp/policies/editorial-policies].
- **Elsevier**: 모든 AI 사용을 공시(disclosure)해야 하며, AI가 생성한 이미지는 원칙적으로 금지한다 [Elsevier Author Policy on AI, 2024].
- **Science**: 가장 엄격 — 편집자의 명시적 허가 없이 생성형 AI 사용을 금지한다 [Thorp, H. H. (2023). ChatGPT is fun, but not an author. *Science*, 379(6630), 313].
- **Nature**: AI를 저자로 인정하지 않으며, Methods에 AI 사용을 공시하라고 요구한다 [Nature Editorial Policy, 2023. https://www.nature.com/nature-portfolio/editorial-policies].

핵심 합의: **AI는 도구이지 저자가 아니다. 책임은 인간에게 있다.**

**(2) 대학: 스펙트럼이 넓지만, "무조건 허용"은 없다**

- **Stanford**: 명시적 허가 없으면 AI 사용은 타인의 도움과 동일하게 취급되며, 과제/시험에서 금지될 수 있다 [Stanford Office of Community Standards, 2025].
- **Harvard**: "responsible experimentation"을 지지하지만, 정보보안, 프라이버시, 저작권, 학문적 진실성을 준수해야 한다. 2025년 가을부터 강의별 AI 정책 명시를 의무화했다 [Harvard Office of Undergraduate Education, Generative AI Guidance, 2025].
- **MIT**: AI를 사용한 표절과 부정행위를 금지하며, 교수자가 과목별 허용 범위를 설정한다 [MIT Academic Integrity Guidelines, 2024].
- **Sydney 대학교**: AI Secure(디지털 기기 금지)와 AI Insecure로 시험을 분류하며, 전체 성적의 30% 이상은 AI Secure로 산출하도록 규정한다 [김성균 교육 문서 인용, Sunghoon Kim 시드니 대학교 사례].

---

### 1-D. 종합: "그대로 쓰면 안 되는 이유"는 정확도와 무관하다

| 삐딱한 주장          | 반론의 핵심                        | 근거                                |
| --------------- | ----------------------------- | --------------------------------- |
| 환각이 줄었다         | 1%의 거짓이 섞인 99%의 참이 더 위험하다     | 김성균 (2025); Bastani et al. (2025) |
| 훑어 보니 괜찮다       | 검증 역량 없이 "괜찮아 보이는 것"은 검증이 아니다 | Mollick & Mollick (2023)          |
| 시간을 절약한다        | 시간은 줄어들었지만 역량도 쌓이이 않는다.       | 김성균 (2025); Stanford policy       |
| 주요 저널도 금지하지 않는다 | "밝히라"는 것이지 "그대로 쓰라"는 것이 아니다   | Nature, Science, Springer Nature  |

**결론적 질문 (답을 주지 않는다)**: AI가 100% 정확해지는 날이 온다면, 그때는 그대로 써도 되는가? — 만약 "된다"면, 연구자의 역할은 무엇이 되는가?

---

## 아젠다 2. 과제·공부·연구에서 AI 활용, 어디까지 인정될 수 있을까?

### 삐딱한 주장 요약

> "내가 책임지면 되지 않나? 학위와 성적이 목적이라면 최선의 도구를 쓰는 게 현명하다. 계산기 비유처럼, AI도 진화한 도구일 뿐이다."

---

### 2-A. 김성균 교수의 입장

**핵심 논지: AI는 Scaffold(발판)여야 하며, Crutch(목발)가 되어서는 안 된다. 그리고 그 구분은 교수자가 아니라 학습자 자신이 판별해야 한다.**

**(1) Scaffold vs. Crutch — 교육 현장에서 확인된 구분**

"Scaffold는 학습자의 사고를 증폭(amplify)하고 점검하는 도구이며, 학습자가 스스로 높은 곳에 도달하도록 돕는 임시 구조물이다. Crutch는 학습자의 사고를 대체(replace)하는 도구이며, 학습자 대신 문제를 해결하는 영구적 의존물이다" [김성균, LLM 활용 교육설계 원칙: Scaffold vs. Crutch, 2025.10.03].

분자독성학 수업에서 한 학생은 "Non-genotoxic carcinogen이 뭔가요?"라고 AI에게 물어 완성된 답을 복사했다(Crutch). 다른 학생은 먼저 교재를 읽고 "내가 이해한 바로는..."이라고 AI에게 확인을 요청했다(Scaffold). 결과물의 표면은 비슷해도, 학습 효과는 근본적으로 달랐다 [김성균, AI 활용 교육혁신 개요서, 2024.12].

**(2) Cognitive Debt는 "나중에" 드러난다**

"Cognitive Debt는 학생이 개념을 충분히 이해하지 못한 채 AI가 제공한 답을 사용할 때 축적되는 '인지적 빚'이다. 당장은 과제를 해결한 것처럼 보이지만, 실제로는 이해 없이 넘어간 상태다" [김성균, AI 활용 교육혁신 FAQ, 2024.12].

Cognitive Debt가 축적되는 세 가지 경로:
1. **ZPD(Zone of Proximal Development, 근접발달영역) 우회**: Vygotsky가 제시한 ZPD는 "혼자서는 못 하지만 도움을 받으면 할 수 있는 영역"으로, 학습이 실제로 일어나는 지점이다. AI가 scaffold로 작동하면 학습자는 이 영역 안에서 성장하지만, AI가 답 자체를 제공하면 학습자는 ZPD를 "통과"한 것이 아니라 "건너뛴" 것이다. 이해 없이 넘어갔으므로 인지부채가 쌓인다.
2. **Productive Struggle(생산적 고투) 시간의 소멸**: 학습은 본질적으로 불편한 과정이다. 스스로 고민하고, 틀리고, 다시 시도하는 시간이 개념을 내재화한다. AI에게 바로 위임하면 이 고투의 시간이 사라지고, 답은 얻되 이해는 축적되지 않는다.
3. **결과물 중심 평가가 만드는 인센티브 왜곡**: 평가가 "어떻게 도달했는가(과정)"가 아니라 "무엇을 제출했는가(산출물)"만 본다면, 학생 입장에서는 최소 노력으로 최대 점수를 얻는 것이 합리적 전략이 된다. 그 결과, AI에게 받아 그대로 제출하는 copy/paste가 학습자의 "최적 전략"으로 작동하게 되며, 평가 시스템 자체가 학습 회피를 유도하는 구조가 된다.

**(3) "불편한 학습환경"이 진짜 교육이다**

"자원이 부족하고 노이즈가 섞여 있고 규칙과 상황은 수시로 바뀌는 가운데 문제해결을 수행하는 것은 불편한 학습환경이었고 앞으로도 AI로도 대체될 수 없는 배움의 과정이어야 할 것이다" [김성균, AI 시대의 교육은 불편한 학습환경이어야, 2026.01.09].

현실의 문제 해결은 교실이나 교과서처럼 정제되어 있지 않다. 예상 밖의 변수가 튀어나오고, 정보는 불완전하며, 상황은 수시로 변한다. 아무리 훈련소에서 철저히 교육받아도 실전 경험이 필요한 것처럼, 연구자도 이런 불확실성 속에서 직접 부딪혀야 역량이 형성된다. AI에 의존하면, 정제된 답만 받게 되어 이 현장 경험마저 부실해진다.

장한나의 사례: 첼로 거장 로스트로포비치는 "첼로 연습실 밖으로 나가라. 미술관에 가고, 숲을 걷고, 사람들의 삶을 관찰해라"고 조언했다. 악보의 기교와 테크닉만으로는 청중의 마음을 울리는 연주를 할 수 없다 — 인간의 삶에 대한 이해와 감성이 음악에 영혼을 불어넣기 때문이다. 연구도 마찬가지이다. AI가 코드를 써주고 문헌을 정리해 주는 것은 기교적 지원이다. 그러나 연구에 통찰을 담으려면 현상에 대한 직접적 관찰, 고민, 불편한 질문이 필요하다 [같은 글].

**(4) 계산기 비유의 한계**

계산기는 계산을 대신한다. AI는 사고를 대신한다. 계산기가 등장한 뒤에도 수학적 사고력 교육은 사라지지 않았다 — 오히려 계산에서 해방된 시간에 개념적 이해와 문제 설계 역량을 키웠다. AI 시대도 마찬가지여야 한다. 문제는 AI가 "개념적 이해"까지 대신하게 되는 순간이다.

---

### 2-B. AI(Claude)의 분석적 관점

**핵심 논지: "내가 책임진다"는 주장은 "책임질 역량"이 전제될 때만 성립한다. 역량 없는 책임 선언은 공허하며, 그 역량은 AI에게 위임할 수 없다.**

**(1) 책임은 능력있는 사람이 지는 것이다**

서울대 가이드라인은 "AI 활용 결과에 대한 모든 책임은 본인에게 있다"고 명시한다. 그런데 책임을 진다는 것은, 산출물의 오류를 **식별하고, 설명하고, 수정할 수 있는 역량**이 있다는 뜻이다. 역량 없이 서명만 하는 것은 책임이 아니라 무모함이다.

환경보건 자료분석 수업에서 AI가 생성한 그래프의 "error bar가 Standard Deviation(표준편차)인지 Standard Error(표준오차)인지"를 묻자 학생이 답하지 못한 사례 [김성균, 교육혁신 개요서, 2024.12]가 이를 보여준다. AI가 만든 코드는 실행되고 그래프도 출력되지만, 그 의미를 설명하지 못하는 사람이 그 결과에 대해 "내가 책임진다"고 말할 수 있는가? 책임의 전제는 이해이다.

**(2) 학위는 역량의 인증이. 산출물은 필요조건이다**  (AI 다루는 테크닉에 학위를 주는 것이 아니다)

학위 과정의 본질은 **과제를 제출하는 것이 아니라, 과제를 수행하는 과정에서 역량을 키우고 인정받는 과정**이다. 논문을 쓰면서 논리적 사고를 훈련하고, 데이터를 분석하면서 방법론적 판단력을 기르고, 선행연구를 읽으면서 비판적 독해력을 체득한다. AI가 이 과정의 실질적 사고를 대신했다면, 학위는 "연구 역량의 인증"이 아니라 "AI 활용 능력의 인증"이 된다.

Kasneci et al. (2023)은 이 위험을 체계적으로 분석했다. AI가 학습자에게 즉각적 답을 제공하면, 학습에 필수적인 인지 과정 — 스스로 문제를 구조화하고, 가설을 세우고, 오류를 통해 수정하는 과정 — 을 우회(bypass)하게 된다. 그 결과 Critical Thinking(비판적 사고)과 Problem-Solving(문제 해결) 역량이 약화된다고 경고했다 [Kasneci, E. et al. (2023). ChatGPT for good? On opportunities and challenges of large language models for education. *Learning and Individual Differences*, 103, 102274]. 학위 과정에서 이 우회가 반복되면, 학위를 받은 뒤에 남는 역량이 무엇인지에 대한 근본적 질문이 제기된다.

**(3) "체득해야 할 것이 달라져야 한다"는 주장 — 맞는 부분과 빠진 부분**

**맞는 부분**: AI가 단순 암기, 반복 코딩, 정형 문서 작성을 더 빠르고 정확하게 수행한다. 체득 대상이 이동해야 한다는 인식은 타당하다.

**빠진 부분**: 체득 대상이 "달라지는" 것이지, "줄어드는" 것이 아니다. AI가 하위 인지 기능(정보 검색, 요약, 번역)을 대신할수록, 교육은 **상위 인지 기능에 더 집중**해야 한다 — 비판적 분석, 연구 설계, 맥락적 해석, 윤리적 판단. 이러한 역량은 편안한 환경이 아니라 **직접 고민하고, 틀리고, 수정하는 불편한 과정**을 통해서만 형성된다.

> UNESCO (2023)는 AI 시대 교육의 핵심 원칙으로 **"인간의 주체성(human agency) 보호"**를 제시했다. AI가 학습자의 인지 발달 자체를 대체해서는 안 되며, 학습자가 AI 산출물을 비판적으로 평가하고 독립적으로 판단하는 역량을 길러야 한다 [UNESCO (2023)]. 핵심 질문은 "무엇을 AI에게 맡길 것인가"가 아니라 **"무엇을 반드시 직접 해야 하는가"**이다.

---

### 2-C. 해외 대학의 접근

- **Harvard Graduate School of Education (HGSE, 2025-2026)**: "responsible experimentation"을 지지하되, **기본값은 금지**이다. 교수자가 별도로 허용하지 않는 한, 생성형 AI로 과제의 전부 또는 일부를 작성하여 제출하는 것은 학문적 진실성 정책(Academic Integrity Policy) 위반이다. 개념 탐색, 브레인스토밍, 사고 정리 등의 보조적 사용은 허용하되, AI 사용 시 도구·프롬프트·활용 방식을 반드시 문서화해야 한다 [HGSE Policy on Student Use of Generative AI in Academic Work, AY 2025-2026. https://registrar.gse.harvard.edu/learning/policies-forms/ai-policy].
- **Stanford**: AI 사용을 타인의 도움(assistance from another person)과 동일하게 취급한다. AI로 과제나 시험을 실질적으로 완성(substantially complete)하는 것은 Honor Code 위반이며, 개별 교수자가 syllabus에서 허용 범위를 설정할 수 있다. 의심스러우면 공시(disclose)가 기본이다 [Stanford Board on Conduct Affairs, Generative AI Policy Guidance, 2023.02.16. https://communitystandards.stanford.edu/generative-ai-policy-guidance].
- **시드니 대학교 (University of Sydney, 2025~)**: Two-Lane 접근법을 도입했다. Lane 1(Secure Assessment)은 대면·감독 하 시험으로 AI 사용이 금지되며, Lane 2(비감독 과제)는 교수자 판단에 따라 AI 허용이 가능하다. 2025년 1학기부터 **기본값은 "시험·수시 테스트 외에는 AI 허용"**으로 전환되었다 [University of Sydney, AI Assessment Policy, 2024.11. https://www.sydney.edu.au/news-opinion/news/2024/11/27/university-of-sydney-ai-assessment-policy.html].

공통점: **AI 사용 자체를 금지하지 않지만, "AI 없이 독립적으로 수행할 수 있는가"를 검증하는 장치(감독 시험, 기본값 금지, 공시 의무)를 반드시 둔다.**

---

### 2-D. 종합

| 삐딱한 주장 | 반론의 핵심 | 근거 |
|---|---|---|
| 내가 책임진다 | 역량 없는 책임은 공허하다 | 김성균 (2024); SNU 가이드라인 |
| 학위가 목적이면 효율적으로 | 학위는 역량의 인증이지 산출물의 인증이 아니다 | Kasneci et al. (2023) |
| 계산기와 같다 | 계산기는 계산을 대신하고, AI는 사고를 대신한다 | UNESCO (2023) |
| 체득 대상이 달라져야 | 맞다, 하지만 비판적 사고는 불편한 과정에서만 형성된다 | 김성균 (2026); Bastani et al. (2025) |

**결론적 질문**: 계산기 비유가 맞다면, AI 시대에 "직접 해야만 하는 것"은 무엇이 남는가? — 그것은 어떻게 훈련되어야 하는가?

---

## 아젠다 3. AI로 논문 쓰기 — 그 경계는 누가 정하는가?

### 삐딱한 주장 요약

> "경계가 모호하다. 밝히면 되는 거 아닌가? Nature도 Science도 금지하지 않는다. 영어 원어민 교정은 허용되는데 AI 교정은 왜 문제인가?"

---

### 3-A. 김성균 교수의 입장

**핵심 논지: 경계가 모호한 것은 사실이다. 그러나 모호하다고 해서 "아무 데나 선을 그어도 된다"는 뜻은 아니다.**

**(1) 원헬스 수업에서 확인된 "AI 요약의 피상성"**

> NEOH(Network for Evaluation of One Health)는 원헬스 접근법의 성과를 체계적으로 평가하기 위한 프레임워크이며, TOC(Theory of Change, 변화이론)는 개입(intervention)이 최종 결과에 이르는 인과 경로를 명시하는 평가 방법론이다. 이 두 개념은 원헬스 수업의 핵심 학습 목표였다.
> 
> "학생들이 AI로 NEOH 프레임워크 4단계를 빠르게 정리했지만, TOC 기반 결과 평가, 비선형적 영향 등 핵심 개념을 피상적으로만 처리했다. 복잡한 시스템 실패의 본질적 이해 없이 형식적 틀 맞추기에 그쳤다" [김성균, 원헬스 수업 AI교육의 역설 사례, 2025].

여기서 "피상적"이란, AI가 각 단계의 **명칭과 정의**는 정확히 제공했지만, 단계 간의 **인과적 연결**(예: 어떤 개입이 왜 비선형적 결과를 낳는지, 시스템 실패가 어디서 시작되는지)에 대한 사고를 학생이 직접 수행하지 않았다는 뜻이다. 형식은 갖춰졌지만 이해는 형성되지 않았다.

단, 이 사례에서 학생만 탓할 수는 없다. 교수자가 AI 활용에 대한 충분한 경험을 갖추고, 학생에게 사전 학습을 요구하며, AI 사용 범위를 명시하지 않았다면 이러한 결과는 예견된 것이다. **교수자의 수업 설계 책임** — AI를 scaffold로 작동시킬 구조를 미리 마련하는 것 — 이 전제되지 않으면, AI 의존의 문제는 학생 개인의 탓으로만 돌릴 수 없다.

논문의 Discussion도 마찬가지다. AI가 초안을 쓰면 형식은 갖춰지지만, 자기 데이터에 대한 깊은 성찰과 선행연구와의 비판적 대화가 빠진다.

**(2) Precautionary Principle(사전주의 원칙) 사례: AI의 단편적 답변이 은폐하는 복잡성**

> "원헬스 강좌에서 학생들은 'PFAS(Per- and Polyfluoroalkyl Substances, 과불화화합물)에 사전주의 원칙 적용'을 AI에게 물은 뒤 '조심스럽게 규제해야 한다'는 단순한 결론에 도달했다. 그러나 왜 특정 용도(소방, 반도체 등)에서 여전히 사용되는지, 대체 기술의 현실적 한계는 무엇인지, 국제 협력은 왜 어려운지에 대한 깊이 있는 탐구를 하지 않았다. AI가 제공하는 명확하고 간결한 답변이 오히려 복합적 사고를 차단하는 역설적 상황이 발생한 것이다" [김성균, 사전주의 원칙에 대한 이해의 예시 - AI 답변의 단편성 폐해, 2025].

논문 쓰기에서도 동일한 위험이 있다. AI가 Discussion을 써주면 "깔끔한 결론"은 나오지만, 연구자만이 알 수 있는 데이터의 한계, 측정의 불확실성, 맥락적 해석이 빠진다. AI는 일반론을 잘 쓰지만, **특수성(specificity)** — 이 데이터에서, 이 맥락에서, 이 한계 하에서 무엇을 말할 수 있는가 — 은 연구자의 몫이다.

**(3) 효율성이 교육적 진정성을 대체할 수 없다**

AI 도구는 정보 접근을 빠르게 하지만, 정보에 빠르게 접근하는 것과 그 정보를 이해하는 것은 다른 차원의 문제이다. 도구를 제공했다고 교육 목표가 달성되는 것이 아니다 [김성균, 원헬스 수업 AI교육의 역설, 핵심 교훈]. 논문 작성에서도 AI가 문헌 정리와 초안 작성의 시간을 줄여주지만, 그 시간에 연구자가 비판적 사고를 수행하지 않으면 효율성은 오히려 피상성의 원인이 된다.

---

### 3-B. AI(Claude)의 분석적 관점

**핵심 논지: "밝히면 된다"는 원칙은 필요조건이지 충분조건이 아니다. 공시(disclosure)는 투명성의 문제이고, 논문의 질은 별개의 문제다.**

**(1) 공시의 스펙트럼 — 같은 "공시"가 아니다**

"AI를 문법 교정에 활용했음"과 "AI를 Discussion 전체 초안에 활용했음"은 같은 수준의 공시가 아니다. Springer Nature는 이를 명확히 구분한다:
- **공시 불요**: "AI-assisted copy editing" — 가독성, 문체, 문법 교정 [Springer Nature Journal Policies, 2024]
- **공시 필요**: AI가 내용을 생성, 초안 작성, 요약하는 등 "creative/editorial role"을 한 경우 [Springer Nature Editorial Policies, 2024]

"학술적 톤으로 다듬어 줘"라는 요청은 이 두 범주의 경계에 있다. AI가 문장 구조와 어휘만 바꿨다면 copy editing(공시 불요)이지만, 논리 연결을 추가하거나 주장의 순서를 재배치했다면 내용 생성(공시 필요)에 해당한다. 그런데 실무적으로, 사용자조차 AI가 어디까지 바꿨는지를 완전히 추적하기 어렵다. 이것이 인간과 기계의 지분을 나누기 어려운 근본적 이유이다.

**(2) 방향이 다른 두 가지 작업 — 표면은 같아도 과정은 다르다**

논문 작성에서 AI를 활용하는 방식은 크게 두 가지로 나뉜다:
- **연구자 주도(Human-first)**: 연구자가 논리 구조와 주장을 먼저 설계하고, AI가 표현을 다듬는다. 사고의 주도권은 연구자에게 있다.
- **AI 주도(AI-first)**: AI가 초안의 논리 구조와 주장 흐름을 생성하고, 연구자가 이를 수정·보완한다. 사고의 출발점이 AI에 있다.

표면적 결과물은 비슷할 수 있다. 그러나 **AI 주도** 방식에서는, 연구자가 AI가 제시한 논리 구조에 Anchoring(고착)될 가능성이 높다. Tversky & Kahneman (1974)이 제시한 Anchoring Bias(정박 편향) — 처음 접한 정보에 판단이 끌려가는 현상 — 가 AI 산출물에도 적용된다. AI가 제시하지 않은 관점, 빠뜨린 반론, 데이터의 미묘한 함의를 연구자가 스스로 떠올리기 어려워지는 것이다 [Tversky, A. & Kahneman, D. (1974). Judgment under Uncertainty: Heuristics and Biases. *Science*, 185(4157), 1124–1131].

이 구분이 중요한 이유는, 같은 수준의 AI 활용이라도 **누가 사고의 출발점을 설정했는가**에 따라 연구의 독창성과 깊이가 근본적으로 달라지기 때문이다.

**(3) "영어 교정과 같다"는 주장이 성립하지 않는 이유**

영어 원어민 교정(proofreading) 서비스는 **내용에 개입하지 않는다**는 명확한 계약 관계 하에서 작동한다. 교정자는 문법, 철자, 어색한 표현을 수정하되, 논리 구조나 주장의 내용을 변경하지 않으며, 변경 이력(tracked changes)이 남아 저자가 모든 수정을 확인·승인할 수 있다.

AI에게 "다듬어 줘"라고 요청하면 상황이 근본적으로 다르다:
- AI는 문법 교정뿐 아니라 **문장 재구성, 논리 연결 추가, 표현의 강화**까지 수행할 수 있다
- 사용자는 AI가 **어디까지** 바꿨는지를 완전히 추적하기 어렵다
- 교정자와 달리, AI에게는 "내용 미개입" 계약이 존재하지 않는다

이 비대칭이 핵심이다. "영어 교정이 허용되니 AI 교정도 같다"는 주장은, 계약 범위가 명확한 인간 교정과 범위가 불분명한 AI 생성을 동일시하는 오류이다.

**(4) 학계의 현재 논쟁: 공시는 의무인가, 자발인가**

AI 사용 공시의 방식에 대해 학계 내에서도 합의가 완성되지 않았다. Tang et al. (2024)은 투명성을 위해 **의무적 공시(mandatory disclosure)**가 필요하다고 주장했다. 반면, Hosseini et al. (2025)은 "writing assistance 목적의 생성형 AI 사용은 자발적 공시(voluntary disclosure)로 충분하다"는 반론을 제기했다 [Hosseini, M. et al. (2025). Disclosing generative AI use for writing assistance should be voluntary. *Research Ethics*. PMC: 12425484].

Hosseini et al.은 초기에는 의무적 공시를 지지했으나 입장을 전환했으며, 그 이유로 ① 문법 교정 수준의 AI 사용까지 공시하면 실효성이 떨어지고, ② 공시 기준이 모호하여 연구자 간 불균형이 발생하며, ③ 영어 비원어민(non-native speaker)에 대한 차별적 효과가 생길 수 있다는 점을 들었다. 이 논쟁 자체가 "밝히면 된다"는 단순한 원칙이 실무에서 얼마나 복잡해지는지를 보여준다.

---

### 3-C. 해외 출판기관의 정책 비교

| 출판기관 | 정책 핵심 | AI 저자 인정 | 공시 요건 |
|---|---|---|---|
| **Nature** | AI 사용 시 Methods에 공시 | 불가 | 필수 |
| **Science** | 편집자 명시적 허가 없으면 금지 | 불가 | 필수 (가장 엄격) |
| **Springer Nature** | Copy editing은 공시 불요, 내용 생성은 공시 필수 | 불가 | 용도별 구분 |
| **Elsevier** | 모든 AI 사용 공시 의무. AI 이미지 원칙적 금지 | 불가 | 필수 (포괄적) |
| **PLOS** | AI 사용 공시 | 불가 | 필수 |
| **서울대 가이드라인** | "가공 없이 그대로" 넣으면 위반. 문법 교정은 예외 | 해당 없음 | 권고 |

[출처: Springer Nature Editorial Policies (2024); Thorp (2023) *Science*; Nature Editorial (2023); Elsevier Author Policy (2024); Kennesaw State University AI Disclosure Requirements 정리 (2024)]

공통점: **"쓰지 마라"가 아니라 "밝혀라". 단, AI는 저자가 될 수 없고, 최종 책임은 인간에게 있다.**

---

### 3-D. 종합

| 삐딱한 주장 | 반론의 핵심 | 근거 |
|---|---|---|
| 경계가 모호하다 | 모호하다고 "아무 데나 선을 그어도 되는 것"은 아니다 | 김성균 (2025 원헬스 사례) |
| 밝히면 된다 | 공시는 투명성이지, 논문 질의 보증이 아니다 | Springer Nature; Tang et al. (2024) |
| 원어민 교정과 같다 | 교정은 내용 미개입 계약, AI는 범위 불분명한 내용 변경 가능 | Springer Nature 용도별 구분; Hosseini et al. (2025) |
| Nature도 금지 안 한다 | "밝히라"이지 "그대로 쓰라"가 아니다. Science는 허가제 | Thorp (2023); Nature (2023) |

**결론적 질문**: 경계가 모호한 것은 사실이다. 그렇다면 여러분은 **자기만의 선을 어디에 긋고 있는가?** 그 선을 의식적으로 정했는가, 아니면 무의식적으로 밀려나고 있는가?

---

## 참고문헌

1. Bastani, H., Bastani, O., Sungu, A., Ge, H., Kabakçı, Ö., & Mariman, R. (2025). Generative AI without guardrails can harm learning. *Proceedings of the National Academy of Sciences*, 122(26). https://doi.org/10.1073/pnas.2422633122
2. Kasneci, E. et al. (2023). ChatGPT for good? On opportunities and challenges of large language models for education. *Learning and Individual Differences*, 103, 102274.
3. Mollick, E. R. & Mollick, L. (2023). Using AI to Implement Effective Teaching Strategies in Classrooms. *The Wharton School Research Paper*. SSRN: 4391243.
4. Thorp, H. H. (2023). ChatGPT is fun, but not an author. *Science*, 379(6630), 313.
5. Tversky, A. & Kahneman, D. (1974). Judgment under Uncertainty: Heuristics and Biases. *Science*, 185(4157), 1124–1131.
6. UNESCO (2023). *Guidance for Generative AI in Education and Research*. Paris: UNESCO.
7. Nature Editorial Policy (2023). Tools such as ChatGPT threaten transparent science. *Nature*, 613, 612.
8. Springer Nature Editorial Policies (2024). https://www.springernature.com/gp/policies/editorial-policies
9. Springer Nature Journal Policies (2024). https://link.springer.com/brands/springer/journal-policies
10. Elsevier Author Policy on Use of Generative AI (2024).
11. Harvard Office of Undergraduate Education (2025). Generative AI Guidance. https://oue.fas.harvard.edu/faculty-resources/generative-ai-guidance/
12. Stanford Office of Community Standards (2025). Generative AI Policy.
13. 서울대학교 (2024). 서울대학교 구성원을 위한 AI 활용 가이드라인.
14. 김성균 (2025). AI 활용 대학원 교육 단상. Obsidian 노트, 2025.08.15.
15. 김성균 (2025). LLM 활용 교육설계 원칙: Scaffold vs. Crutch. Obsidian 노트, 2025.10.03.
16. 김성균 (2024). AI 활용 대학원 교육혁신 FAQ. 환경보건학과 내부 자료.
17. 김성균 (2024). AI 활용 교육혁신 개요서 v2. 환경보건학과 내부 자료.
18. 김성균 (2026). AI 시대의 교육은 불편한 학습환경이어야. Obsidian 노트, 2026.01.09.
19. 김성균 (2025). 원헬스 수업당시 AI교육의 역설 사례. Obsidian 노트.
20. 김성균 (2025). 사전주의 원칙에 대한 이해의 예시 - AI 답변의 단편성 폐해. Obsidian 노트.
21. Tang, A., Li, K-K., Kwok, K.O. et al. (2024). The importance of transparency: Declaring the use of generative artificial intelligence (AI) in academic writing.
22. Hosseini, M. et al. (2025). Disclosing generative AI use for writing assistance should be voluntary. *Research Ethics*. https://pmc.ncbi.nlm.nih.gov/articles/PMC12425484/
